{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽµ Music Genre Clustering - Feature Extraction & Analysis\n",
    "\n",
    "## Deep Mathematical Implementation and Exploratory Data Analysis\n",
    "\n",
    "**Project:** Music Genre Clustering using K-Means & GMM  \n",
    "**Dataset:** GTZAN (1,000 songs, 10 genres)  \n",
    "**Author:** Vedant  \n",
    "**Date:** October 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook covers:\n",
    "1. Audio feature extraction (5 features)\n",
    "2. Mathematical foundations of each feature\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Feature correlation analysis\n",
    "5. Data preprocessing and standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_context('notebook', font_scale=1.2)\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LIBRARY VERSIONS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"NumPy:     {np.__version__}\")\n",
    "print(f\"Pandas:    {pd.__version__}\")\n",
    "print(f\"Librosa:   {librosa.__version__}\")\n",
    "print(f\"Matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn:   {sns.__version__}\")\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… All libraries loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Background of Audio Features\n",
    "\n",
    "### 2.1 Feature Definitions\n",
    "\n",
    "#### **1. Tempo (BPM)**\n",
    "\n",
    "Tempo measures the speed of music in beats per minute:\n",
    "\n",
    "$$\\text{Tempo (BPM)} = \\frac{60 \\times \\text{Sample Rate}}{\\text{Hop Length} \\times \\text{Beat Period}}$$\n",
    "\n",
    "Extracted using librosa's beat tracking algorithm which:\n",
    "1. Computes onset strength envelope\n",
    "2. Applies autocorrelation to find periodicity\n",
    "3. Estimates tempo from dominant period\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Energy (RMS)**\n",
    "\n",
    "Root Mean Square energy measures signal intensity:\n",
    "\n",
    "$$E_{RMS}(t) = \\sqrt{\\frac{1}{N} \\sum_{n=0}^{N-1} |x[n]|^2}$$\n",
    "\n",
    "Where:\n",
    "- $x[n]$ = audio signal samples\n",
    "- $N$ = frame length\n",
    "- Higher RMS = louder, more energetic sound\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Loudness (RMS)**\n",
    "\n",
    "For GTZAN dataset, loudness is equivalent to energy (RMS mean):\n",
    "\n",
    "$$\\text{Loudness} = \\frac{1}{T} \\sum_{t=1}^{T} E_{RMS}(t)$$\n",
    "\n",
    "Where $T$ = total number of frames\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Valence (Approximated)**\n",
    "\n",
    "Valence represents musical positiveness/happiness. Approximated using:\n",
    "\n",
    "$$\\text{Valence} \\approx \\frac{SC_{norm} + H_{norm}}{2}$$\n",
    "\n",
    "Where:\n",
    "- $SC_{norm} = \\frac{\\text{Spectral Centroid}}{4000}$ (normalized)\n",
    "- $H_{norm} = \\frac{\\text{Harmonic Content}}{0.5}$ (normalized)\n",
    "\n",
    "**Spectral Centroid:**\n",
    "$$SC = \\frac{\\sum_{k=1}^{K} f(k) \\cdot |X(k)|}{\\sum_{k=1}^{K} |X(k)|}$$\n",
    "\n",
    "Represents the \"center of mass\" of the spectrum (brightness)\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Danceability (Approximated)**\n",
    "\n",
    "Danceability measures how suitable a track is for dancing:\n",
    "\n",
    "$$\\text{Danceability} \\approx \\frac{T_{norm} + ZCR_{mean}}{2}$$\n",
    "\n",
    "Where:\n",
    "- $T_{norm} = \\frac{\\text{Tempo}}{200}$ (normalized by max expected BPM)\n",
    "- $ZCR_{mean}$ = Zero Crossing Rate (mean)\n",
    "\n",
    "**Zero Crossing Rate:**\n",
    "$$ZCR = \\frac{1}{N-1} \\sum_{n=1}^{N-1} \\mathbb{1}_{x[n] \\cdot x[n-1] < 0}$$\n",
    "\n",
    "Measures how often the signal crosses zero amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Comprehensive audio feature extraction for music clustering.\n",
    "    \n",
    "    Features:\n",
    "    ---------\n",
    "    1. Tempo (BPM) - Beat tracking\n",
    "    2. Energy (RMS) - Root Mean Square\n",
    "    3. Loudness (RMS) - Mean energy\n",
    "    4. Valence - Spectral centroid + harmonic content\n",
    "    5. Danceability - Tempo + zero crossing rate\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_dir : str or Path\n",
    "        Directory containing GTZAN audio files\n",
    "    duration : int, default=30\n",
    "        Duration to analyze (seconds)\n",
    "    sr : int, default=22050\n",
    "        Sample rate (Hz)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, audio_dir, duration=30, sr=22050):\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.duration = duration\n",
    "        self.sr = sr\n",
    "        print(f\"\\nðŸ“ Audio directory: {self.audio_dir}\")\n",
    "        print(f\"â±ï¸  Duration: {self.duration}s\")\n",
    "        print(f\"ðŸŽ¼ Sample rate: {self.sr} Hz\\n\")\n",
    "        \n",
    "    def extract_tempo(self, y, sr):\n",
    "        \"\"\"\n",
    "        Extract tempo using beat tracking.\n",
    "        \n",
    "        Algorithm:\n",
    "        ----------\n",
    "        1. Compute onset strength envelope\n",
    "        2. Apply autocorrelation\n",
    "        3. Find dominant periodic component\n",
    "        4. Convert to BPM\n",
    "        \"\"\"\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        return float(tempo)\n",
    "    \n",
    "    def extract_energy_loudness(self, y):\n",
    "        \"\"\"\n",
    "        Extract RMS energy/loudness.\n",
    "        \n",
    "        Formula:\n",
    "        --------\n",
    "        RMS = sqrt(mean(x^2))\n",
    "        \"\"\"\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        return float(np.mean(rms))\n",
    "    \n",
    "    def extract_valence(self, y, sr):\n",
    "        \"\"\"\n",
    "        Approximate valence from spectral and harmonic features.\n",
    "        \n",
    "        Components:\n",
    "        -----------\n",
    "        1. Spectral Centroid (brightness) - higher = brighter/happier\n",
    "        2. Harmonic Content - stronger harmonics = more musical/positive\n",
    "        \n",
    "        Formula:\n",
    "        --------\n",
    "        valence = (centroid_norm + harmonic_norm) / 2\n",
    "        \"\"\"\n",
    "        # Spectral centroid\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        centroid_mean = np.mean(spectral_centroid)\n",
    "        centroid_norm = centroid_mean / 4000.0  # Normalize (typical max ~4000 Hz)\n",
    "        \n",
    "        # Harmonic component\n",
    "        harmonic, _ = librosa.effects.hpss(y)\n",
    "        harmonic_mean = np.mean(np.abs(harmonic))\n",
    "        harmonic_norm = harmonic_mean / 0.5  # Normalize (typical max ~0.5)\n",
    "        \n",
    "        # Combine\n",
    "        valence = (centroid_norm + harmonic_norm) / 2.0\n",
    "        return float(valence)\n",
    "    \n",
    "    def extract_danceability(self, y, sr, tempo):\n",
    "        \"\"\"\n",
    "        Approximate danceability from tempo and rhythm.\n",
    "        \n",
    "        Components:\n",
    "        -----------\n",
    "        1. Tempo - faster = more danceable\n",
    "        2. Zero Crossing Rate - rhythmic complexity\n",
    "        \n",
    "        Formula:\n",
    "        --------\n",
    "        danceability = (tempo_norm + zcr_mean) / 2\n",
    "        \"\"\"\n",
    "        # Tempo component\n",
    "        tempo_norm = tempo / 200.0  # Normalize (typical max ~200 BPM)\n",
    "        \n",
    "        # Zero crossing rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        zcr_mean = float(np.mean(zcr))\n",
    "        \n",
    "        # Combine\n",
    "        danceability = (tempo_norm + zcr_mean) / 2.0\n",
    "        return float(danceability)\n",
    "    \n",
    "    def extract_single_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Extract all 5 features from a single audio file.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(file_path, duration=self.duration, sr=self.sr)\n",
    "            \n",
    "            # Extract features\n",
    "            tempo = self.extract_tempo(y, sr)\n",
    "            energy = self.extract_energy_loudness(y)\n",
    "            loudness = energy  # Same as energy for GTZAN\n",
    "            valence = self.extract_valence(y, sr)\n",
    "            danceability = self.extract_danceability(y, sr, tempo)\n",
    "            \n",
    "            return {\n",
    "                'filename': file_path.name,\n",
    "                'genre': file_path.parent.name,\n",
    "                'tempo': tempo,\n",
    "                'energy': energy,\n",
    "                'loudness': loudness,\n",
    "                'valence': valence,\n",
    "                'danceability': danceability\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {file_path.name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_all(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Extract features from entire dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_path : str or Path, optional\n",
    "            Path to save extracted features CSV\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame : Features for all songs\n",
    "        \"\"\"\n",
    "        features_list = []\n",
    "        audio_files = sorted(list(self.audio_dir.glob('**/*.wav')))\n",
    "        \n",
    "        print(f\"\\nðŸŽµ Found {len(audio_files)} audio files\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXTRACTING FEATURES\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        for audio_file in tqdm(audio_files, desc=\"Processing\", unit=\"file\"):\n",
    "            features = self.extract_single_file(audio_file)\n",
    "            if features:\n",
    "                features_list.append(features)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(features_list)\n",
    "        \n",
    "        # Handle missing values (zero-imputation)\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        print(f\"\\nâœ… Extraction complete!\")\n",
    "        print(f\"   Total songs: {len(df)}\")\n",
    "        print(f\"   Total features: {len(df.columns) - 2}\\n\")  # Exclude filename and genre\n",
    "        \n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            save_path = Path(save_path)\n",
    "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            df.to_csv(save_path, index=False)\n",
    "            print(f\"ðŸ’¾ Saved to: {save_path}\\n\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"âœ… AudioFeatureExtractor class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Features from GTZAN Dataset\n",
    "\n",
    "**Note:** Uncomment and run the cell below if you have GTZAN dataset locally.  \n",
    "Otherwise, load pre-extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTION 1: Extract features from raw audio (if you have GTZAN dataset)\n",
    "# extractor = AudioFeatureExtractor(\n",
    "#     audio_dir='data/gtzan/genres_original',\n",
    "#     duration=30,\n",
    "#     sr=22050\n",
    "# )\n",
    "\n",
    "# features_df = extractor.extract_all(save_path='data/processed/features_selected.csv')\n",
    "\n",
    "# OPTION 2: Load pre-extracted features\n",
    "features_df = pd.read_csv('../data/processed/features_selected.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET LOADED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Shape: {features_df.shape}\")\n",
    "print(f\"Columns: {features_df.columns.tolist()}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Display first 10 rows\n",
    "features_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 5.1 Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "feature_cols = ['tempo', 'energy', 'loudness', 'valence', 'danceability']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Compute statistics\n",
    "stats = features_df[feature_cols].describe()\n",
    "print(stats)\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADDITIONAL STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for col in feature_cols:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(f\"  Range: [{features_df[col].min():.4f}, {features_df[col].max():.4f}]\")\n",
    "    print(f\"  IQR: {features_df[col].quantile(0.75) - features_df[col].quantile(0.25):.4f}\")\n",
    "    print(f\"  Variance: {features_df[col].var():.4f}\")\n",
    "    print(f\"  Skewness: {features_df[col].skew():.4f}\")\n",
    "    print(f\"  Kurtosis: {features_df[col].kurtosis():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Missing Values and Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "missing = features_df.isnull().sum()\n",
    "print(missing)\n",
    "print(f\"\\nTotal missing values: {missing.sum()}\")\n",
    "\n",
    "# Duplicates\n",
    "duplicates = features_df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "\n",
    "# Genre distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENRE DISTRIBUTION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "genre_counts = features_df['genre'].value_counts().sort_index()\n",
    "print(genre_counts)\n",
    "\n",
    "# Check balance\n",
    "print(f\"\\nDataset balance:\")\n",
    "print(f\"  Min: {genre_counts.min()} songs\")\n",
    "print(f\"  Max: {genre_counts.max()} songs\")\n",
    "print(f\"  Balanced: {'âœ… Yes' if genre_counts.std() < 5 else 'âŒ No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature Distributions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive distribution plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "fig.suptitle('Feature Distributions with Statistical Annotations', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "for idx, feature in enumerate(feature_cols):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Histogram\n",
    "    n, bins, patches = ax.hist(features_df[feature], bins=40, \n",
    "                                edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    \n",
    "    # Statistical lines\n",
    "    mean_val = features_df[feature].mean()\n",
    "    median_val = features_df[feature].median()\n",
    "    std_val = features_df[feature].std()\n",
    "    \n",
    "    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {mean_val:.3f}')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', linewidth=2, \n",
    "               label=f'Median: {median_val:.3f}')\n",
    "    ax.axvline(mean_val + std_val, color='orange', linestyle=':', linewidth=1.5,\n",
    "               label=f'Â±1Ïƒ: {std_val:.3f}')\n",
    "    ax.axvline(mean_val - std_val, color='orange', linestyle=':', linewidth=1.5)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title(f'{feature.capitalize()} Distribution', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel(feature.capitalize(), fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3, linestyle=':')\n",
    "\n",
    "# Remove last empty subplot\n",
    "fig.delaxes(axes[2, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Distribution plots generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Box Plots for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "fig.suptitle('Box Plots - Outlier Detection', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(feature_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create box plot\n",
    "    box_data = ax.boxplot([features_df[feature]], \n",
    "                           labels=[feature.capitalize()],\n",
    "                           patch_artist=True,\n",
    "                           notch=True,\n",
    "                           showmeans=True,\n",
    "                           meanprops=dict(marker='D', markerfacecolor='red', markersize=8))\n",
    "    \n",
    "    # Color boxes\n",
    "    for patch in box_data['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    \n",
    "    # Calculate outliers\n",
    "    Q1 = features_df[feature].quantile(0.25)\n",
    "    Q3 = features_df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = features_df[(features_df[feature] < lower_bound) | \n",
    "                          (features_df[feature] > upper_bound)][feature]\n",
    "    \n",
    "    # Annotations\n",
    "    ax.set_ylabel('Value', fontsize=10)\n",
    "    ax.set_title(f'{len(outliers)} outliers', fontsize=10, color='red')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Box plots generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = features_df[feature_cols].corr()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION MATRIX\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            fmt='.3f',\n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True, \n",
    "            linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"},\n",
    "            vmin=-1, \n",
    "            vmax=1,\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret correlations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find strong correlations (|r| > 0.7)\n",
    "strong_corr = []\n",
    "for i in range(len(feature_cols)):\n",
    "    for j in range(i+1, len(feature_cols)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            strong_corr.append((feature_cols[i], feature_cols[j], corr_val))\n",
    "\n",
    "if strong_corr:\n",
    "    print(\"\\nStrong Correlations (|r| > 0.7):\")\n",
    "    for feat1, feat2, corr in strong_corr:\n",
    "        print(f\"  {feat1} â†” {feat2}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No strong correlations found - features are independent!\")\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(f\"  â€¢ Energy-Loudness correlation: {correlation_matrix.loc['energy', 'loudness']:.4f} (Expected: ~1.0)\")\n",
    "print(f\"  â€¢ Tempo-Danceability correlation: {correlation_matrix.loc['tempo', 'danceability']:.4f}\")\n",
    "print(f\"  â€¢ Low correlations indicate good feature diversity for clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Pair Plot - Relationships Between Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pair plot (sample subset for performance)\n",
    "sample_size = min(1000, len(features_df))\n",
    "sample_df = features_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(f\"\\nðŸ“Š Generating pair plot for {sample_size} samples...\\n\")\n",
    "\n",
    "# Pair plot\n",
    "pairplot = sns.pairplot(sample_df, \n",
    "                         vars=feature_cols,\n",
    "                         hue='genre',\n",
    "                         diag_kind='kde',\n",
    "                         plot_kws={'alpha': 0.6, 's': 30},\n",
    "                         height=2.5)\n",
    "\n",
    "pairplot.fig.suptitle('Feature Relationships by Genre', \n",
    "                       y=1.01, fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Pair plot generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Genre-wise Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean features per genre\n",
    "genre_means = features_df.groupby('genre')[feature_cols].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEAN FEATURES BY GENRE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(genre_means)\n",
    "\n",
    "# Visualize with heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "sns.heatmap(genre_means.T, \n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cmap='YlOrRd',\n",
    "            linewidths=1,\n",
    "            cbar_kws={\"label\": \"Feature Value\"},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Average Feature Values by Genre', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Genre', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Genre analysis heatmap generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing for Clustering\n",
    "\n",
    "### 6.1 Standardization (Z-score Normalization)\n",
    "\n",
    "**Mathematical Formula:**\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $x$ = original feature value\n",
    "- $\\mu$ = mean of feature\n",
    "- $\\sigma$ = standard deviation of feature\n",
    "- $z$ = standardized value (mean=0, std=1)\n",
    "\n",
    "**Why Standardization?**\n",
    "1. Features have different scales (Tempo: 50-200, Energy: 0-0.5)\n",
    "2. K-Means uses Euclidean distance - sensitive to scale\n",
    "3. Standardization ensures equal contribution from all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize features\n",
    "X = features_df[feature_cols].values\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STANDARDIZATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create DataFrame for scaled features\n",
    "scaled_df = pd.DataFrame(X_scaled, columns=[f'{col}_scaled' for col in feature_cols])\n",
    "\n",
    "# Verify standardization\n",
    "print(\"\\nOriginal Statistics:\")\n",
    "print(features_df[feature_cols].describe().loc[['mean', 'std']])\n",
    "\n",
    "print(\"\\nStandardized Statistics:\")\n",
    "print(scaled_df.describe().loc[['mean', 'std']])\n",
    "\n",
    "print(\"\\nâœ… Standardization complete!\")\n",
    "print(\"   All features now have mean â‰ˆ 0 and std â‰ˆ 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed features\n",
    "output_path = Path('data/processed/features_standardized.csv')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Combine original metadata with scaled features\n",
    "processed_df = pd.concat([\n",
    "    features_df[['filename', 'genre']],\n",
    "    features_df[feature_cols],\n",
    "    scaled_df\n",
    "], axis=1)\n",
    "\n",
    "processed_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Processed data saved to: {output_path}\")\n",
    "print(f\"   Shape: {processed_df.shape}\")\n",
    "print(f\"   Columns: {processed_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Key Findings\n",
    "\n",
    "### Dataset Overview\n",
    "- **Total Songs:** 1,000\n",
    "- **Genres:** 10 (balanced)\n",
    "- **Features:** 5 audio characteristics\n",
    "- **Missing Values:** None\n",
    "\n",
    "### Feature Characteristics\n",
    "1. **Tempo:** Wide range (50-200 BPM), relatively uniform distribution\n",
    "2. **Energy/Loudness:** Low to moderate values, right-skewed\n",
    "3. **Valence:** Moderate values, approximately normal distribution\n",
    "4. **Danceability:** Moderate to high values, slightly left-skewed\n",
    "\n",
    "### Correlation Insights\n",
    "- Energy and Loudness highly correlated (r â‰ˆ 1.0) - expected\n",
    "- Other features show low correlation - good for clustering\n",
    "- Features provide complementary information\n",
    "\n",
    "### Next Steps\n",
    "1. Apply K-Means clustering (Notebook 2)\n",
    "2. Apply GMM clustering (Notebook 3)\n",
    "3. Perform PCA dimensionality reduction (Notebook 4)\n",
    "4. Evaluate clustering performance (Notebook 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
