{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽµ Music Genre Clustering - K-Means Implementation\n",
    "\n",
    "## Deep Mathematical Analysis and Implementation\n",
    "\n",
    "**Project:** Music Genre Clustering using K-Means Algorithm  \n",
    "**Dataset:** GTZAN (1,000 songs, 10 genres)  \n",
    "**Author:** Vedant  \n",
    "**Date:** October 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook covers:\n",
    "1. K-Means algorithm mathematical foundations\n",
    "2. Implementation and training\n",
    "3. Elbow method for optimal K\n",
    "4. Cluster analysis and interpretation\n",
    "5. Centroid analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundation of K-Means\n",
    "\n",
    "### 1.1 Optimization Objective\n",
    "\n",
    "K-Means solves the following optimization problem:\n",
    "\n",
    "$$\\min_{C} J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$$\n",
    "\n",
    "Where:\n",
    "- $C = \\{C_1, C_2, ..., C_k\\}$ = Set of k clusters\n",
    "- $\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x$ = Centroid of cluster $C_i$\n",
    "- $\\|x - \\mu_i\\|^2$ = Squared Euclidean distance\n",
    "- $J$ = Within-Cluster Sum of Squares (WCSS)\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Algorithm Steps\n",
    "\n",
    "**Initialization (Random):**\n",
    
    "For n_init = 20 iterations:\n",
    "    1. Randomly select k data points as initial centroids\n",
    "    2. Run Lloyd's algorithm\n",
    "    3. Keep best result (lowest WCSS)\n",
    "```\n",
    "\n",
    "**Step 1 - Assignment Step:**\n",
    "\n",
    "Assign each point to nearest centroid:\n",
    "\n",
    "$$C_i^{(t)} = \\{x_p : \\|x_p - \\mu_i^{(t)}\\|^2 \\leq \\|x_p - \\mu_j^{(t)}\\|^2 \\text{ for all } j=1,...,k\\}$$\n",
    "\n",
    "**Step 2 - Update Step:**\n",
    "\n",
    "Recalculate centroids:\n",
    "\n",
    "$$\\mu_i^{(t+1)} = \\frac{1}{|C_i^{(t)}|} \\sum_{x \\in C_i^{(t)}} x$$\n",
    "\n",
    "**Convergence:**\n",
    "\n",
    "Repeat until:\n",
    "$$|\\text{WCSS}^{(t+1)} - \\text{WCSS}^{(t)}| < \\epsilon \\text{ or } t > \\text{max_iter}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Distance Calculation\n",
    "\n",
    "**Euclidean Distance in 5D Space:**\n",
    "\n",
    "For standardized features $x = [tempo, energy, loudness, valence, danceability]^T$:\n",
    "\n",
    "$$d(x, \\mu_i) = \\sqrt{\\sum_{j=1}^{5} (x_j - \\mu_{ij})^2}$$\n",
    "\n",
    "Expanded:\n",
    "$$d(x, \\mu_i) = \\sqrt{(tempo_x - tempo_{\\mu_i})^2 + (energy_x - energy_{\\mu_i})^2 + (loudness_x - loudness_{\\mu_i})^2 + (valence_x - valence_{\\mu_i})^2 + (danceability_x - danceability_{\\mu_i})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed features\n",
    "features_df = pd.read_csv('data/processed/features_selected.csv')\n",
    "feature_cols = ['tempo', 'energy', 'loudness', 'valence', 'danceability']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET LOADED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Shape: {features_df.shape}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Genres: {features_df['genre'].nunique()}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing - Standardization\n",
    "\n",
    "### Why Standardization?\n",
    "\n",
    "K-Means uses Euclidean distance, which is sensitive to feature scales:\n",
    "\n",
    "**Problem:** \n",
    "- Tempo: 50-200 (large range)\n",
    "- Energy: 0-0.5 (small range)\n",
    "- Without scaling, tempo dominates distance calculation\n",
    "\n",
    "**Solution:**\n",
    "$$z_j = \\frac{x_j - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "After standardization:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "- All features contribute equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature matrix\n",
    "X = features_df[feature_cols].values\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ORIGINAL DATA STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nShape: {X.shape}\")\n",
    "print(f\"\\nMean per feature:\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"  {col}: {X[:, i].mean():.4f}\")\n",
    "print(f\"\\nStd per feature:\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"  {col}: {X[:, i].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STANDARDIZED DATA STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nShape: {X_scaled.shape}\")\n",
    "print(f\"\\nMean per feature (should be ~0):\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"  {col}: {X_scaled[:, i].mean():.6f}\")\n",
    "print(f\"\\nStd per feature (should be ~1):\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"  {col}: {X_scaled[:, i].std():.6f}\")\n",
    "\n",
    "print(\"\\nâœ… Standardization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elbow Method - Finding Optimal K\n",
    "\n",
    "### Within-Cluster Sum of Squares (WCSS)\n",
    "\n",
    "Also called **Inertia**:\n",
    "\n",
    "$$\\text{WCSS}(k) = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$$\n",
    "\n",
    "**Properties:**\n",
    "- Decreases as k increases\n",
    "- k=n â†’ WCSS=0 (each point is its own cluster)\n",
    "- **Elbow point** = optimal k (diminishing returns)\n",
    "\n",
    "### How to Find Elbow:\n",
    "Look for point where WCSS decrease rate slows significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different values of k\n",
    "k_range = range(2, 21)\n",
    "wcss_values = []\n",
    "silhouette_values = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ELBOW METHOD - TESTING K VALUES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for k in k_range:\n",
    "    # Train K-Means\n",
    "    kmeans = KMeans(n_clusters=k, n_init=20, max_iter=300, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    wcss = kmeans.inertia_\n",
    "    silhouette = silhouette_score(X_scaled, labels)\n",
    "    \n",
    "    wcss_values.append(wcss)\n",
    "    silhouette_values.append(silhouette)\n",
    "    \n",
    "    print(f\"k={k:2d} | WCSS: {wcss:8.2f} | Silhouette: {silhouette:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Elbow method analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot elbow curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# WCSS plot\n",
    "ax1.plot(k_range, wcss_values, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.axvline(x=10, color='red', linestyle='--', linewidth=2, label='Chosen k=10')\n",
    "ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\n",
    "ax1.set_title('Elbow Method - WCSS vs k', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "# Silhouette plot\n",
    "ax2.plot(k_range, silhouette_values, 'go-', linewidth=2, markersize=8)\n",
    "ax2.axvline(x=10, color='red', linestyle='--', linewidth=2, label='Chosen k=10')\n",
    "ax2.axhline(y=0, color='black', linestyle=':', linewidth=1)\n",
    "ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax2.set_title('Silhouette Score vs k', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Elbow curve and Silhouette plot generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Chosen k = 10:**\n",
    "1. Matches number of original genres (10)\n",
    "2. Elbow point visible around k=8-10\n",
    "3. Reasonable Silhouette score\n",
    "4. Interpretable number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Means Training with k=10\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    
    "KMeans(\n",
    "    n_clusters=10,        # Number of clusters\n",
    "    n_init=20,           # Number of initializations (best kept)\n",
    "    max_iter=300,        # Maximum iterations per run\n",
    "    random_state=42      # Reproducibility\n",
    ")\n",
    "```\n",
    "\n",
    "### Training Process\n",
    "\n",
    "1. Run Lloyd's algorithm 20 times with different initializations\n",
    "2. Each run: iterate until convergence (max 300 iterations)\n",
    "3. Keep the run with lowest WCSS\n",
    "4. Return final centroids and cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final K-Means model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING K-MEANS MODEL\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=10,\n",
    "    n_init=20,\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"âœ… Training complete!\")\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  n_clusters: {kmeans.n_clusters}\")\n",
    "print(f\"  n_init: {kmeans.n_init}\")\n",
    "print(f\"  max_iter: {kmeans.max_iter}\")\n",
    "print(f\"  n_iter: {kmeans.n_iter_} (actual iterations)\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Final WCSS: {kmeans.inertia_:.2f}\")\n",
    "print(f\"  Cluster centers shape: {kmeans.cluster_centers_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to dataframe\n",
    "features_df['kmeans_cluster'] = labels\n",
    "\n",
    "# Cluster distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTER DISTRIBUTION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "cluster_counts = pd.Series(labels).value_counts().sort_index()\n",
    "print(cluster_counts)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "cluster_counts.plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n",
    "ax.axhline(y=100, color='red', linestyle='--', linewidth=2, label='Balanced (100 songs)')\n",
    "ax.set_xlabel('Cluster ID', fontsize=12)\n",
    "ax.set_ylabel('Number of Songs', fontsize=12)\n",
    "ax.set_title('K-Means Cluster Distribution', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCluster Balance:\")\n",
    "print(f\"  Min: {cluster_counts.min()} songs (Cluster {cluster_counts.idxmin()})\")\n",
    "print(f\"  Max: {cluster_counts.max()} songs (Cluster {cluster_counts.idxmax()})\")\n",
    "print(f\"  Mean: {cluster_counts.mean():.1f} songs\")\n",
    "print(f\"  Std: {cluster_counts.std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Centroid Analysis\n",
    "\n",
    "### Understanding Centroids\n",
    "\n",
    "Each centroid $\\mu_i$ represents the \"average song\" in cluster $i$:\n",
    "\n",
    "$$\\mu_i = [\\mu_{tempo}, \\mu_{energy}, \\mu_{loudness}, \\mu_{valence}, \\mu_{danceability}]^T$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Songs close to centroid â†’ typical of cluster\n",
    "- Songs far from centroid â†’ outliers or boundary cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centroids in original scale\n",
    "centroids_scaled = kmeans.cluster_centers_\n",
    "centroids_original = scaler.inverse_transform(centroids_scaled)\n",
    "\n",
    "# Create DataFrame\n",
    "centroids_df = pd.DataFrame(\n",
    "    centroids_original,\n",
    "    columns=feature_cols,\n",
    "    index=[f'Cluster {i}' for i in range(10)]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTER CENTROIDS (ORIGINAL SCALE)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(centroids_df)\n",
    "\n",
    "# Visualize centroids\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "sns.heatmap(centroids_df.T, \n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cmap='RdYlGn',\n",
    "            center=centroids_df.values.mean(),\n",
    "            linewidths=1,\n",
    "            cbar_kws={\"label\": \"Feature Value\"},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Cluster Centroids - Feature Profiles', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Cluster', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Centroid heatmap generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid Interpretation\n",
    "\n",
    "Analyze what each cluster represents based on centroid features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find distinctive features per cluster\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTER CHARACTERISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for cluster_id in range(10):\n",
    "    print(f\"\\nðŸŽµ Cluster {cluster_id}:\")\n",
    "    \n",
    "    # Get centroid features\n",
    "    centroid = centroids_df.iloc[cluster_id]\n",
    "    \n",
    "    # Find highest/lowest features\n",
    "    max_feat = centroid.idxmax()\n",
    "    max_val = centroid.max()\n",
    "    min_feat = centroid.idxmin()\n",
    "    min_val = centroid.min()\n",
    "    \n",
    "    print(f\"   Songs: {cluster_counts[cluster_id]}\")\n",
    "    print(f\"   Highest: {max_feat} = {max_val:.2f}\")\n",
    "    print(f\"   Lowest: {min_feat} = {min_val:.2f}\")\n",
    "    \n",
    "    # Categorize based on tempo and energy\n",
    "    tempo_cat = \"Fast\" if centroid['tempo'] > 120 else \"Slow\"\n",
    "    energy_cat = \"High Energy\" if centroid['energy'] > 0.15 else \"Low Energy\"\n",
    "    \n",
    "    print(f\"   Profile: {tempo_cat}, {energy_cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distance from Centroid Analysis\n",
    "\n",
    "### Distance Metric\n",
    "\n",
    "For each song $x$ in cluster $C_i$:\n",
    "\n",
    "$$d(x, \\mu_i) = \\sqrt{\\sum_{j=1}^{5} (x_j - \\mu_{ij})^2}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Small distance** â†’ Song is representative of cluster\n",
    "- **Large distance** â†’ Song is atypical (boundary case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances from centroids\n",
    "distances = np.linalg.norm(X_scaled - centroids_scaled[labels], axis=1)\n",
    "features_df['distance_from_centroid'] = distances\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISTANCE FROM CENTROID STATISTICS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"Overall:\")\n",
    "print(f\"  Mean: {distances.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(distances):.4f}\")\n",
    "print(f\"  Std: {distances.std():.4f}\")\n",
    "print(f\"  Min: {distances.min():.4f}\")\n",
    "print(f\"  Max: {distances.max():.4f}\")\n",
    "\n",
    "print(f\"\\nPer Cluster:\")\n",
    "for cluster_id in range(10):\n",
    "    cluster_distances = features_df[features_df['kmeans_cluster'] == cluster_id]['distance_from_centroid']\n",
    "    print(f\"  Cluster {cluster_id}: {cluster_distances.mean():.4f} Â± {cluster_distances.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distance distributions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "fig.suptitle('Distance from Centroid Distribution per Cluster', fontsize=16, fontweight='bold')\n",
    "\n",
    "for cluster_id in range(10):\n",
    "    row = cluster_id // 5\n",
    "    col = cluster_id % 5\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    cluster_data = features_df[features_df['kmeans_cluster'] == cluster_id]\n",
    "    \n",
    "    ax.hist(cluster_data['distance_from_centroid'], bins=20, \n",
    "            edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    ax.axvline(cluster_data['distance_from_centroid'].mean(), \n",
    "               color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax.set_title(f'Cluster {cluster_id}', fontweight='bold')\n",
    "    ax.set_xlabel('Distance', fontsize=9)\n",
    "    ax.set_ylabel('Count', fontsize=9)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Distance distribution plots generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cluster-Genre Mapping\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Analyze how original genres map to discovered clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "confusion_matrix = pd.crosstab(\n",
    "    features_df['genre'], \n",
    "    features_df['kmeans_cluster'],\n",
    "    margins=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENRE vs CLUSTER CONFUSION MATRIX\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Remove margins row/col for heatmap\n",
    "conf_matrix_plot = confusion_matrix.iloc[:-1, :-1]\n",
    "\n",
    "sns.heatmap(conf_matrix_plot, \n",
    "            annot=True, \n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"label\": \"Number of Songs\"},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Genre vs K-Means Cluster Mapping', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Cluster ID', fontsize=12)\n",
    "ax.set_ylabel('Original Genre', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Confusion matrix heatmap generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Genre per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOMINANT GENRE PER CLUSTER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for cluster_id in range(10):\n",
    "    cluster_data = features_df[features_df['kmeans_cluster'] == cluster_id]\n",
    "    genre_counts = cluster_data['genre'].value_counts()\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_data)} songs):\")\n",
    "    print(f\"  Dominant: {genre_counts.index[0]} ({genre_counts.iloc[0]} songs, {genre_counts.iloc[0]/len(cluster_data)*100:.1f}%)\")\n",
    "    print(f\"  Top 3:\")\n",
    "    for i in range(min(3, len(genre_counts))):\n",
    "        pct = genre_counts.iloc[i]/len(cluster_data)*100\n",
    "        print(f\"    {i+1}. {genre_counts.index[i]}: {genre_counts.iloc[i]} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save K-Means Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(kmeans, 'models/kmeans_model.pkl')\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "\n",
    "# Save cluster assignments\n",
    "features_df.to_csv('data/processed/kmeans_cluster_assignments.csv', index=False)\n",
    "\n",
    "# Save centroids\n",
    "centroids_df.to_csv('data/processed/kmeans_centroids.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILES SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ… models/kmeans_model.pkl\")\n",
    "print(\"âœ… models/scaler.pkl\")\n",
    "print(\"âœ… data/processed/kmeans_cluster_assignments.csv\")\n",
    "print(\"âœ… data/processed/kmeans_centroids.csv\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### K-Means Results\n",
    "\n",
    "**Model Configuration:**\n",
    "- k = 10 clusters\n",
    "- n_init = 20 (best of 20 runs)\n",
    "- max_iter = 300\n",
    "- Actual iterations: ~10-15 (converged quickly)\n",
    "\n",
    "**Cluster Distribution:**\n",
    "- Relatively balanced (40-150 songs per cluster)\n",
    "- No empty clusters\n",
    "- Reasonable variance in cluster sizes\n",
    "\n",
    "**Key Findings:**\n",
    "1. Clusters capture musical characteristics beyond genre labels\n",
    "2. Some genres split across multiple clusters (diversity within genre)\n",
    "3. Some clusters contain multiple genres (similarity across genres)\n",
    "4. Centroids show interpretable feature patterns\n",
    "\n",
    "### Next Steps\n",
    "1. Compare with GMM clustering (Notebook 3)\n",
    "2. Apply PCA for visualization (Notebook 4)\n",
    "3. Detailed evaluation metrics (Notebook 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
